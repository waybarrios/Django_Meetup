{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ![alt text](images/piensa_logo.png \"Logo Title Text 1\")\n",
    "\n",
    "## Daisy: Sentiment Analysis Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "#### ** Introduce myself\n",
    "#### ** Jupyter Notebook\n",
    "#### ** DAISY ALGORITHM\n",
    "#### ** TF IDF Transform\n",
    "#### ** Support Vector Machines\n",
    "#### ** Neuronal Networks\n",
    "\n",
    "<img src=\"images/saudi_meme.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#python libs\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import time \n",
    "import scipy\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = r'/Users/waybarrios/Documents/SVM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ['pos','neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Hashtags\n",
    "    hash_regex = re.compile(r\"#(\\w+)\")\n",
    "    def hash_repl(match):\n",
    "        return '__HASH_'+match.group(1).upper()\n",
    "\n",
    "    # Handels\n",
    "    hndl_regex = re.compile(r\"@(\\w+)\")\n",
    "    def hndl_repl(match):\n",
    "        return '__HNDL'#_'+match.group(1).upper()\n",
    "    # URLs\n",
    "    url_regex = re.compile(r\"(http|https|ftp)://[a-zA-Z0-9\\./]+\")\n",
    "\n",
    "    # Spliting by word boundaries\n",
    "    word_bound_regex = re.compile(r\"\\W+\")\n",
    "\n",
    "    # Repeating words like hurrrryyyyyy\n",
    "    rpt_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE);\n",
    "    def rpt_repl(match):\n",
    "        return match.group(1)+match.group(1)\n",
    "\n",
    "    # Emoticons\n",
    "    emoticons = \\\n",
    "        [('__EMOT_SMILEY',[':-)', ':)', '(:', '(-:', ] ),\\\n",
    "            ('__EMOT_LAUGH',[':-D', ':D', 'X-D', 'XD', 'xD', ] ),\\\n",
    "            ('__EMOT_LOVE',['<3', ':\\*', ] ),\\\n",
    "            ('__EMOT_WINK',[';-)', ';)', ';-D', ';D', '(;', '(-;', ] ),\\\n",
    "            ('__EMOT_FROWN',[':-(', ':(', '(:', '(-:', ] ),\\\n",
    "            ('__EMOT_CRY',[':,(', ':\\'(', ':\"(', ':(('] ),\\\n",
    "        ]\n",
    "\n",
    "    # Punctuations\n",
    "    punctuations = \\\n",
    "        [ #('',['.', ] ),\\\n",
    "            #('',[',', ] ),\\\n",
    "            #('',['\\'', '\\\"', ] ),\\\n",
    "            ('__PUNC_EXCL',['!', '¡', ] ),\\\n",
    "            ('__PUNC_QUES',['?', '¿', ] ),\\\n",
    "            ('__PUNC_ELLP',['...', '…', ] ),\\\n",
    "            #FIXME : MORE? http://en.wikipedia.org/wiki/Punctuation\n",
    "        ]\n",
    "        \n",
    "    #For emoticon regexes\n",
    "    def escape_paren(arr):\n",
    "        return [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
    "\n",
    "    def regex_union(arr):\n",
    "        return '(' + '|'.join( arr ) + ')'\n",
    "\n",
    "    emoticons_regex = [ (repl, re.compile(regex_union(escape_paren(regx))) ) \\\n",
    "                        for (repl, regx) in emoticons ]\n",
    "\n",
    "    #For punctuation replacement\n",
    "    def punctuations_repl(match):\n",
    "        text = match.group(0)\n",
    "        repl = []\n",
    "        for (key, parr) in punctuations :\n",
    "            for punc in parr :\n",
    "                if punc in text:\n",
    "                    repl.append(key)\n",
    "        if( len(repl)>0 ) :\n",
    "            return ' '+' '.join(repl)+' '\n",
    "        else :\n",
    "            return ' '\n",
    "\n",
    "    def processHashtags(text, subject='', query=[]):\n",
    "        return re.sub( hash_regex, hash_repl, text )\n",
    "\n",
    "    def processHandles(text, subject='', query=[]):\n",
    "        return re.sub( hndl_regex, hndl_repl, text )\n",
    "\n",
    "    def processUrls(text, subject='', query=[]):\n",
    "        return re.sub( url_regex, ' __URL ', text )\n",
    "\n",
    "    def processEmoticons(text, subject='', query=[]):\n",
    "        for (repl, regx) in emoticons_regex :\n",
    "            text = re.sub(regx, ' '+repl+' ', text)\n",
    "        return text\n",
    "\n",
    "    def processPunctuations( text, subject='', query=[]):\n",
    "        return re.sub( word_bound_regex , punctuations_repl, text )\n",
    "\n",
    "    def processRepeatings( \ttext, subject='', query=[]):\n",
    "        return re.sub( rpt_regex, rpt_repl, text )\n",
    "\n",
    "    def processQueryTerm( \ttext, subject='', query=[]):\n",
    "        query_regex = \"|\".join([ re.escape(q) for q in query])\n",
    "        return re.sub( query_regex, '__QUER', text, flags=re.IGNORECASE )\n",
    "\n",
    "    def countHandles(text):\n",
    "        return len( re.findall( hndl_regex, text) )\n",
    "    def countHashtags(text):\n",
    "        return len( re.findall( hash_regex, text) )\n",
    "    def countUrls(text):\n",
    "        return len( re.findall( url_regex, text) )\n",
    "    def countEmoticons(text):\n",
    "        count = 0\n",
    "        for (repl, regx) in emoticons_regex :\n",
    "            count += len( re.findall( regx, text) )\n",
    "        return count\n",
    "\n",
    "    #FIXME: preprocessing.preprocess()! wtf! will need to move.\n",
    "    #FIXME: use process functions inside\n",
    "    def processAll(text, subject='', query=[]):\n",
    "\n",
    "        if(len(query)>0):\n",
    "            query_regex = \"|\".join([ re.escape(q) for q in query])\n",
    "            text = re.sub( query_regex, '__QUER', text, flags=re.IGNORECASE )\n",
    "\n",
    "        text = re.sub( hash_regex, hash_repl, text )\n",
    "        text = re.sub( hndl_regex, hndl_repl, text )\n",
    "        text = re.sub( url_regex, ' __URL ', text )\n",
    "\n",
    "        for (repl, regx) in emoticons_regex :\n",
    "            text = re.sub(regx, ' '+repl+' ', text)\n",
    "\n",
    "\n",
    "        text = text.replace('\\'','')\n",
    "        # FIXME: Jugad\n",
    "\n",
    "        text = re.sub( word_bound_regex , punctuations_repl, text )\n",
    "        text = re.sub( rpt_regex, rpt_repl, text )\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_files = ['train-neg.txt','train-pos.txt']\n",
    "test_files = ['test-neg.txt','test-pos.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fname in train_files:\n",
    "    with open(os.path.join(data_dir,fname),'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        if 'neg' in fname:\n",
    "            for row in reader:\n",
    "                row_new = ''.join(row)\n",
    "                row_process=processAll(row_new)\n",
    "                #print(row_process)\n",
    "                train_data.append(row[0])\n",
    "                train_labels.append(0)\n",
    "        else:\n",
    "            for row in reader:\n",
    "                row_new = ''.join(row)\n",
    "                row_process=processAll(row_new)\n",
    "                train_data.append(row[0])\n",
    "                train_labels.append(1)  \n",
    "                \n",
    "for fname in test_files:\n",
    "    with open(os.path.join(data_dir,fname),'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        if 'neg' in fname:\n",
    "            for row in reader:\n",
    "                row_new = ''.join(row)\n",
    "                row_process=processAll(row_new)\n",
    "                test_data.append(row[0])\n",
    "                test_labels.append(0)\n",
    "        else:\n",
    "            for row in reader:\n",
    "                row_new = ''.join(row)\n",
    "                row_process=processAll(row_new)\n",
    "                test_data.append(row[0])\n",
    "                test_labels.append(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_data = np.concatenate((train_data[0:100000],train_data[-100000:]))\n",
    "sub_labels = np.concatenate((train_labels[0:100000],train_labels[-100000:]))\n",
    "\n",
    "sub_test = np.concatenate((test_data[0:1875],test_data[-1875:]))\n",
    "sub_tlabel = np.concatenate((test_labels[0:1875],test_labels[-1875:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Transform\n",
    "\n",
    "<p>In information retrieval, tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.</p>\n",
    "\n",
    "<p>Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.</p>\n",
    "\n",
    "<p>One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf = True,\n",
    "                           use_idf = True)\n",
    "#print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vectors = vectorizer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "### Definition: \n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, the method is likely to give poor performances.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n",
    "\n",
    "### Kernel Classification: \n",
    "![SVM Classification](images/plot_iris_0012.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 11.485854\n"
     ]
    }
   ],
   "source": [
    "tt1 = time.time()\n",
    "classifier_liblinear = svm.LinearSVC()\n",
    "classifier_liblinear.fit(train_vectors,train_labels)\n",
    "tt2 = time.time()\n",
    "print \"training time: %f\" %(tt2-tt1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.75      0.76      7500\n",
      "          1       0.75      0.77      0.76      7500\n",
      "\n",
      "avg / total       0.76      0.76      0.76     15000\n",
      "\n",
      "testing time: 0.456012\n"
     ]
    }
   ],
   "source": [
    "tt3 = time.time()\n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "prediction_liblinear = classifier_liblinear.predict(test_vectors)\n",
    "tt4 = time.time()\n",
    "print(classification_report(test_labels,prediction_liblinear))\n",
    "print \"testing time: %f\" %(tt4-tt3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 462.770762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "t0 = time.time()\n",
    "clf = SVC(C=1.2, kernel='linear', degree=3, gamma='auto', coef0=0.0, \n",
    "          shrinking=True, probability=False, tol=0.00001, cache_size=850, \n",
    "          class_weight=None, verbose=False, max_iter=10000, \n",
    "          decision_function_shape=None, random_state=None)\n",
    "clf.fit(train_vectors, train_labels)\n",
    "t1 = time.time()\n",
    "print \"training time: %f\" %(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "#test_vectors = vectorizer.transform(sub_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM SKLEARN PERFORMANCE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.49      0.54      7500\n",
      "          1       0.57      0.68      0.62      7500\n",
      "\n",
      "avg / total       0.59      0.58      0.58     15000\n",
      "\n",
      "testing time: 21.657634\n"
     ]
    }
   ],
   "source": [
    " t2 = time.time()\n",
    "print \"SVM SKLEARN PERFORMANCE\"\n",
    "trad = clf.predict(test_vectors)\n",
    "t3 = time.time()\n",
    "print(classification_report(test_labels,trad))\n",
    "print \"testing time: %f\" %(t3-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_vectors.toarray()\n",
    "xt = test_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tensorflow](images/logo-tensor.png)\n",
    "\n",
    "TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.\n",
    "\n",
    "<p>Github profile: https://github.com/tensorflow/tensorflow </p>\n",
    "<p>Docs: https://www.tensorflow.org/versions/r0.9/api_docs/index.html </p>\n",
    "<p>Tutorials: https://www.tensorflow.org/versions/r0.9/tutorials/index.html </p>\n",
    "<p>Wayner's examples:</p> \n",
    "* Tensorflow CNN: https://github.com/waybarrios/TensorFlow_CNN\n",
    "* CNN vs Softmax: https://github.com/waybarrios/CNN_vs_Softmax_Tensorflow\n",
    "* MNIST EXAMPLE TENSORFLOW: https://github.com/waybarrios/Easily-Example-TensorFLow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skflow\n",
    "from sklearn import datasets, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlowLinearClassifier class is deprecated. Please consider using LinearClassifier as an alternative.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skflow training time: 113.183056\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = skflow.TensorFlowLinearClassifier(n_classes=2)\n",
    "t4 = time.time()\n",
    "classifier.fit(np.array(X), np.array(sub_labels))\n",
    "t5 = time.time()\n",
    "print \"skflow training time: %f\" %(t5-t4)\n",
    "skflow_pred = classifier.predict(xt)\n",
    "t6 = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSORFLOW SVM PERFORMANCE\n",
      "skflow testing time: 5.682661\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.63      0.67      7500\n",
      "          1       0.67      0.76      0.71      7500\n",
      "\n",
      "avg / total       0.69      0.69      0.69     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"TENSORFLOW SVM PERFORMANCE\"\n",
    "print \"skflow testing time: %f\" %(t6-t5)\n",
    "print(classification_report(test_labels,skflow_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deep_meme.png\" alt=\"meme\" style=\"width: 560px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronal Networks\n",
    "In machine learning and cognitive science, artificial neural networks (ANNs) are a family of models inspired by biological neural networks (the central nervous systems of animals, in particular the brain) which are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown. Artificial neural networks are typically specified using three things:\n",
    "\n",
    " **Architecture** specifies what variables are involved in the network and their topological relationships—for example the variables involved in a neural network might be the weights of the connections between the neurons, along with activities of the neurons\n",
    "\n",
    "\n",
    "\n",
    "<p> **Activity Rule** Most neural network models have short time-scale dynamics: local rules define how the activities of the neurons change in response to each other. Typically the activity rule depends on the weights (the parameters) in the network. </p>\n",
    "\n",
    "<p> **Learning Rule** The learning rule specifies the way in which the neural network's weights change with time. This learning is usually viewed as taking place on a longer time scale than the time scale of the dynamics under the activity rule. Usually the learning rule will depend on the activities of the neurons. It may also depend on the values of the target values supplied by a teacher and on the current value of the weights.There are three major learning paradigms, each corresponding to a particular abstract learning task. These are **supervised learning**, **unsupervised learning** and **reinforcement learning**.</p>\n",
    "\n",
    "* Supervised learning:In supervised learning, we are given a set of example pairs ${\\displaystyle \\textstyle (x,y),x\\in X,y\\in Y} \\textstyle (x,y),x\\in X,y\\in Y$ and the aim is to find a function ${\\displaystyle \\textstyle f:X\\rightarrow Y} $ in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.\n",
    "* Unsupervised Learning: In unsupervised learning, some data ${\\displaystyle \\textstyle x}$ is given and the cost function to be minimized, that can be any function of the data ${\\displaystyle \\textstyle x} $ and the network's output, ${\\displaystyle \\textstyle f}$. The cost function is dependent on the task (what we are trying to model) and our a priori assumptions (the implicit properties of our model, its parameters and the observed variables).\n",
    "\n",
    "* Reinforcement learning: In reinforcement learning, data ${\\displaystyle \\textstyle x}$ are usually not given, but generated by an agent's interactions with the environment. At each point in time ${\\displaystyle \\textstyle t}$ , the agent performs an action ${\\displaystyle \\textstyle y_{t}}$  and the environment generates an observation ${\\displaystyle \\textstyle x_{t}}$  and an instantaneous cost ${\\displaystyle \\textstyle c_{t}}$ , according to some (usually unknown) dynamics. The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost, e.g., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.\n",
    "\n",
    "\n",
    "**Recommendation Stanford Course: http://cs231n.github.io/ **\n",
    "\n",
    "### Gradient Descent \n",
    "\n",
    "Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent is also known as steepest descent, or the method of steepest descent.\n",
    "\n",
    "\n",
    "![Tensorflow](images/gradient.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Gradient descent is based on the observation that if the multi-variable function ${\\displaystyle F(\\mathbf {x} )}$ is defined and differentiable in a neighborhood of a point ${\\displaystyle \\mathbf {a} } $ , then ${\\displaystyle F(\\mathbf {x} )}$ decreases fastest if one goes from ${\\displaystyle \\mathbf {a} } $  in the direction of the negative gradient of $ F$ at ${\\displaystyle \\mathbf {a} }  , {\\displaystyle -\\nabla F(\\mathbf {a} )} $. \n",
    "<p>It follows that, if: </p>\n",
    "\n",
    "$${\\displaystyle \\mathbf {b} =\\mathbf {a} -\\gamma \\nabla F(\\mathbf {a} )}$$\n",
    "\n",
    "### Optimization Algorithms for Gradient Descent\n",
    "\n",
    "![Tensorflow](images/updater.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlowDNNClassifier class is deprecated. Please consider using DNNClassifier as an alternative.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN training time: 92.069817\n",
      "DNN testing time: 3.664607\n",
      "Accuracy: 0.500533\n"
     ]
    }
   ],
   "source": [
    "t7 = time.time()\n",
    "clas_nn = skflow.TensorFlowDNNClassifier(hidden_units=[10,20], n_classes=\n",
    "clas_nn.fit(np.array(X), np.array(sub_labels))\n",
    "t8 = time.time()\n",
    "print \"DNN training time: %f\" %(t8 - t7)\n",
    "nn_pred = clas_nn.predict(xt)\n",
    "t9 = time.time()\n",
    "print \"DNN testing time: %f\" %(t9 - t8)\n",
    "score = metrics.accuracy_score(test_labels,nn_pred)\n",
    "print(\"Accuracy: %f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuronal Networks Performance\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.00      0.00      7500\n",
      "          1       0.50      1.00      0.67      7500\n",
      "\n",
      "avg / total       0.70      0.50      0.33     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Neuronal Networks Performance\"\n",
    "print(classification_report(test_labels,nn_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/deep_toy.png\" alt=\"meme_toy\" style=\"width: 560px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
